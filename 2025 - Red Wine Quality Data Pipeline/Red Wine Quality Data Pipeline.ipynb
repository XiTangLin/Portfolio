{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0b306b4-f953-46ef-b6b3-a9ef0474c60e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **Red Wine Quality Data Pipeline**\n",
    "## Summary\n",
    "In this project I:\n",
    "- Learned how to use AWS S3, Databricks, and PySpark.\n",
    "- Setup a Data Pipeline from AWS S3 to Databricks.\n",
    "- Created a Histogram to Explore the Distribution of Wine Quality\n",
    "- Used PySpark MLlib to Build and Evaluate Linear Regression, Random Forest, and Gradient Boosted Tree Models\n",
    "- Investigated the Worse than Expected Performance of GBT and Tuned Parameters.\n",
    "- Interpreted the Models and the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4807ffe-8d5c-48a9-9048-44dac67a4a28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Setup AWS S3, Databricks, and Data\n",
    "1. Created an AWS Account, an S3 Bucket, and Uploaded Data to the Bucket\n",
    "2. Created a Databricks Account, connecting it to AWS and Github.\n",
    "3. Imported Libraries and Loaded Data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "417d6a91-1466-4b05-beb6-906748331bef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor, GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fca4b253-2fa5-4a94-9d6b-e91f198376ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Getting Data from S3\n",
    "df = spark.read.csv(\"s3://red-wine-quality-data-pipeline/raw/winequality/winequality-red.csv\",header=True,inferSchema=True)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f52c2c4-f141-4cf3-b123-1a1780340779",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##2. Exploratory Data Analysis\n",
    "1. Displayed the Data in a Dataframe\n",
    "2. Created a Histogram to Visualize Wine Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6d22877-f1ab-48b3-8fe9-8e4f42ea7322",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display the Data\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2aeab6ae-ab31-44a0-91c5-73724ef7ab53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating a Histogram to Visualize the Distribution of Wine Quality.\n",
    "plt.hist(df.select('quality').toPandas()['quality'], bins=np.arange(0.5,10.5,1), edgecolor='black')\n",
    "plt.title('Distribution of Wine Quality')\n",
    "plt.xlabel('Quality')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "908cc1c6-4a5a-4db1-afbc-c3dcdff0da00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Build Machine Learning Models\n",
    "1. Used an Assembler to Prepare the Data for PySpark\n",
    "2. Split the Data into Training and Test\n",
    "3. Built a Linear Regression, Random Forest, and Gradient Boosted Trees Model using PySpark MLlib.\n",
    "4. Evaluated and Printed the Results\n",
    "5. Experimented with different maxIters for GBT\n",
    "\n",
    "**Notes:**\n",
    "Linear Regression: Y = B_0 + B_1\\*X_1 + B_2\\*X_2 + ...\n",
    "- No Interaction Terms\n",
    "Random Forest: Takes the average prediction of Multiple Decision Trees trained on different subsets of the data.\n",
    "Gradient Boosted Trees: Trains an Initial Tree and then Trains additional Trees on the error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd3dfc88-20fc-418e-a731-19df61bf7fcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Collapse the Features into a Vector so PySpark can use it.\n",
    "assembler = VectorAssembler(inputCols=df.drop('quality').columns, outputCol='raw_features')\n",
    "df2 = assembler.transform(df)\n",
    "\n",
    "# Standardize the Data so LR's Coefficients are Comparable Intra-Model\n",
    "scaler = StandardScaler(inputCol='raw_features', outputCol='features', withStd=True, withMean=True)\n",
    "scaler_model = scaler.fit(df2)\n",
    "df2 = scaler_model.transform(df2)\n",
    "\n",
    "# Train Test Split\n",
    "train_df, test_df = df2.randomSplit([0.8, 0.2], seed=9)\n",
    "\n",
    "# Initialize Evaluator\n",
    "rmse_evaluator = RegressionEvaluator(labelCol='quality', predictionCol='prediction', metricName='rmse')\n",
    "r2_evaluator = RegressionEvaluator(labelCol='quality', predictionCol='prediction', metricName='r2')\n",
    "\n",
    "# 1. Linear Regression\n",
    "lr = LinearRegression(featuresCol='features', labelCol='quality')\n",
    "lr_model = lr.fit(train_df)\n",
    "lr_predictions = lr_model.transform(test_df)\n",
    "lr_rmse = rmse_evaluator.evaluate(lr_predictions)\n",
    "lr_r2 = r2_evaluator.evaluate(lr_predictions)\n",
    "\n",
    "# 2. Random Forest\n",
    "rf = RandomForestRegressor(featuresCol='features', labelCol='quality', numTrees=20)\n",
    "rf_model = rf.fit(train_df)\n",
    "rf_predictions = rf_model.transform(test_df)\n",
    "rf_rmse = rmse_evaluator.evaluate(rf_predictions)\n",
    "rf_r2 = r2_evaluator.evaluate(rf_predictions)\n",
    "\n",
    "# 3. Gradient Boosted Trees\n",
    "gbt = GBTRegressor(featuresCol='features', labelCol='quality', maxIter=10)\n",
    "gbt_model = gbt.fit(train_df)\n",
    "gbt_predictions = gbt_model.transform(test_df)\n",
    "gbt_rmse = rmse_evaluator.evaluate(gbt_predictions)\n",
    "gbt_r2 = r2_evaluator.evaluate(gbt_predictions)\n",
    "\n",
    "# Compare Results\n",
    "print(\"\\n=== Model Comparison ===\")\n",
    "print(\"[RMSE]\")\n",
    "print(f\"Linear Regression: {lr_rmse:.4f}\")\n",
    "print(f\"Random Forest: {rf_rmse:.4f}\")\n",
    "print(f\"GBT: {gbt_rmse:.4f}\")\n",
    "print(\"[R2]\")\n",
    "print(f\"Linear Regression: {lr_r2:.4f}\")\n",
    "print(f\"Random Forest: {rf_r2:.4f}\")\n",
    "print(f\"GBT: {gbt_r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17f38b62-d02c-4d08-9a1b-b940c5a14f6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Note: I noticed GBT had significantly lower than expected performance so I decided to look at its performance on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec66d3fb-af03-48cf-991d-64636b711d3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# How do the models perform on the training data?\n",
    "# 1. Linear Regression\n",
    "lr_predictions_train = lr_model.transform(train_df)\n",
    "lr_r2_train = r2_evaluator.evaluate(lr_predictions_train)\n",
    "\n",
    "# 2. Random Forest\n",
    "rf_predictions_train = rf_model.transform(train_df)\n",
    "rf_r2_train = r2_evaluator.evaluate(rf_predictions_train)\n",
    "\n",
    "# 3. Gradient Boosted Trees\n",
    "gbt_predictions_train = gbt_model.transform(train_df)\n",
    "gbt_r2_train = r2_evaluator.evaluate(gbt_predictions_train)\n",
    "\n",
    "# Compare Results\n",
    "print(\"\\n=== Model Training Data Performance ===\")\n",
    "print(\"[R2]\")\n",
    "print(f\"Linear Regression: {lr_r2_train:.4f}\")\n",
    "print(f\"Random Forest: {rf_r2_train:.4f}\")\n",
    "print(f\"GBT: {gbt_r2_train:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af9fe7f4-7e92-4857-857c-c07938f30353",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Comment: It looks like it may have overfit the data. What if I changed the number of trees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe74d0b0-8b43-4238-869d-2e0d69c16547",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#MaxIter = 20\n",
    "gbt20 = GBTRegressor(featuresCol='features', labelCol='quality', maxIter=20)\n",
    "gbt20_model = gbt20.fit(train_df)\n",
    "gbt20_predictions = gbt20_model.transform(test_df)\n",
    "gbt20_r2 = r2_evaluator.evaluate(gbt20_predictions)\n",
    "\n",
    "#MaxIter = 5\n",
    "gbt5 = GBTRegressor(featuresCol='features', labelCol='quality', maxIter=5)\n",
    "gbt5_model = gbt5.fit(train_df)\n",
    "gbt5_predictions = gbt5_model.transform(test_df)\n",
    "gbt5_r2 = r2_evaluator.evaluate(gbt5_predictions)\n",
    "\n",
    "\n",
    "print(\"\\n=== Model Comparison ===\")\n",
    "print(\"[R2]\")\n",
    "print(f\"GBT: {gbt_r2:.4f}\")\n",
    "print(f\"GBT20: {gbt20_r2:.4f}\")\n",
    "print(f\"GBT5: {gbt5_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebf2f147-8e83-44e5-a6bf-0c4d31d4a86a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "It looks like the original model was the best! It's possible to find the best value for maxIter (number of trees used). However, based on what we've seen so far, I believe the other models to be more suitable for predicting wine quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3ce60bb-987a-4d6d-a219-26df11b25319",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4.Results\n",
    "The models all point to alcohol being the most useful predictor of wine quality.\n",
    "\n",
    "The Linear Regression model expects one standard deviation increase in alcohol to lead to a 0.30 increase in rating.\n",
    "\n",
    "The Random Forest Model and Gradient Boosted Trees Model both agree with alochol accounting for 33% and 15% of the predictive power of the model.\n",
    "\n",
    "While GBT has the highest predictive power on the training data, it performs poorly on the test data. After examining the models and tuning the parameters, I believe it is overfitting the training data due to the following factors:\n",
    "- Small Dataset Size -> Easy for GBT to memorize individual wines.\n",
    "- More Noise and Less Signal -> Subjective Ratings\n",
    "- Imbalanced Classes -> GBT memorizes the edge cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30089d33-38cb-416c-91a7-59c08a482b20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Look at Model Coefficients\n",
    "feature_names = df.drop('quality').columns\n",
    "\n",
    "# Linear Regression Coefficients\n",
    "lr_coef_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Linear Regression Coefficient': [round(c, 4) for c in lr_model.coefficients]\n",
    "})\n",
    "print('Linear Regression Coefficients')\n",
    "display(lr_coef_df)\n",
    "\n",
    "# Random Forest Feature Importances\n",
    "rf_imp_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Random Forest Importance': [round(f, 4) for f in rf_model.featureImportances.toArray()]\n",
    "})\n",
    "print('Random Forest Feature Importances')\n",
    "display(rf_imp_df)\n",
    "\n",
    "# GBT Feature Importances\n",
    "gbt_imp_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'GBT Importance': [round(f, 4) for f in gbt_model.featureImportances.toArray()]\n",
    "})\n",
    "print('GBT Feature Importances')\n",
    "display(gbt_imp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77490780-eb4e-4453-9621-afc3ed994926",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Data Source:\n",
    "https://www.kaggle.com/datasets/uciml/red-wine-quality-cortez-et-al-2009?resource=download\n",
    "AI Disclosure:\n",
    "Used Databricks' automated code completion, Claude for code suggestions, and ChatGPT to brainstorm ideas."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Red Wine Quality Data Pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
